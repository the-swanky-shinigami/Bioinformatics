{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split VCF File by Column Value\n",
    "\n",
    "Overview\n",
    "\n",
    "This script processes a VCF (Variant Call Format) file, ensures all rows have the same number of columns, and then splits the file into multiple parts based on the values in the 20th column. Each split dataset is saved as a separate .vcf file.\n",
    "\n",
    "Features\n",
    "\n",
    "* Reads a large VCF file without skipping any data.\n",
    "\n",
    "* Ensures irregular rows with extra/missing columns are properly handled.\n",
    "\n",
    "* Identifies unique values in the 20th column.\n",
    "\n",
    "* Splits the data into multiple DataFrames based on those values.\n",
    "\n",
    "* Saves each subset as a new VCF file in a separate folder.\n",
    "\n",
    "Requirements\n",
    "\n",
    "* Python 3.7+\n",
    "\n",
    "* Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"Your_vcf_file_name_here.vcf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read the file and find max column count\n",
    "max_columns = 0\n",
    "rows = []\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.strip().split(\"\\t\")  # Split each line by tab\n",
    "        max_columns = max(max_columns, len(values))  # Track max column count\n",
    "        rows.append(values)  # Store the row data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Adjust the header to match max columns\n",
    "header = rows[0]  # First row is the header\n",
    "extra_cols_needed = max_columns - len(header)\n",
    "\n",
    "if extra_cols_needed > 0:\n",
    "    header += [f\"Extra_{i+1}\" for i in range(extra_cols_needed)]  # Add missing columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Convert all rows to match max column count\n",
    "normalized_rows = [row + [\"\"] * (max_columns - len(row)) for row in rows[1:]]  # Fill missing columns with empty strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create DataFrame\n",
    "df = pd.DataFrame(normalized_rows, columns=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          rs# alleles chrom   pos strand assembly# center protLSID assayLSID  \\\n",
      "0  1pos1178.1     G/T     1  1178      +        NA     NA       NA        NA   \n",
      "1  1pos1203.1     T/C     1  1203      +        NA     NA       NA        NA   \n",
      "2  1pos1249.1     A/C     1  1249      +        NA     NA       NA        NA   \n",
      "3  1pos1266.1     G/A     1  1266      +        NA     NA       NA        NA   \n",
      "4  1pos1277.1     T/C     1  1277      +        NA     NA       NA        NA   \n",
      "\n",
      "  panel  ...                 Extra_5 Extra_6 Extra_7    Extra_8 Extra_9  \\\n",
      "0    NA  ...                                                              \n",
      "1    NA  ...                                                              \n",
      "2    NA  ...                                                              \n",
      "3    NA  ...  CHR_START-Os01g0100100                  n.1266G>A           \n",
      "4    NA  ...  CHR_START-Os01g0100100                  n.1277T>C           \n",
      "\n",
      "  Extra_10 Extra_11 Extra_12 Extra_13 Extra_14  \n",
      "0                                               \n",
      "1                                               \n",
      "2                                               \n",
      "3                                               \n",
      "4                                               \n",
      "\n",
      "[5 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.head())  # Verify the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Identify unique values in the 20th column (index 19)\n",
    "column_20_name = df.columns[19]  # Get column name\n",
    "unique_values = df[column_20_name].unique()  # List unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['intergenic_region', 'downstream_gene_variant',\n",
       "       '5_prime_UTR_variant', 'intron_variant', 'upstream_gene_variant',\n",
       "       'missense_variant', 'synonymous_variant',\n",
       "       'non_coding_transcript_exon_variant', '3_prime_UTR_variant',\n",
       "       'stop_gained', 'splice_region_variant&synonymous_variant',\n",
       "       'splice_region_variant&intron_variant',\n",
       "       'splice_acceptor_variant&intron_variant', 'stop_lost',\n",
       "       'start_lost', 'missense_variant&splice_region_variant',\n",
       "       'splice_donor_variant&intron_variant', 'stop_retained_variant',\n",
       "       'stop_lost&splice_region_variant',\n",
       "       'splice_region_variant&non_coding_transcript_exon_variant',\n",
       "       'splice_donor_variant&splice_region_variant&intron_variant',\n",
       "       'stop_gained&splice_region_variant',\n",
       "       'splice_region_variant&stop_retained_variant',\n",
       "       'initiator_codon_variant'], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Split and save each part\n",
    "output_dir = \"split_vcf_files\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # Create directory if not exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: split_vcf_files\\split_intergenic_region.vcf\n",
      "Saved: split_vcf_files\\split_downstream_gene_variant.vcf\n",
      "Saved: split_vcf_files\\split_5_prime_UTR_variant.vcf\n",
      "Saved: split_vcf_files\\split_intron_variant.vcf\n",
      "Saved: split_vcf_files\\split_upstream_gene_variant.vcf\n",
      "Saved: split_vcf_files\\split_missense_variant.vcf\n",
      "Saved: split_vcf_files\\split_synonymous_variant.vcf\n",
      "Saved: split_vcf_files\\split_non_coding_transcript_exon_variant.vcf\n",
      "Saved: split_vcf_files\\split_3_prime_UTR_variant.vcf\n",
      "Saved: split_vcf_files\\split_stop_gained.vcf\n",
      "Saved: split_vcf_files\\split_splice_region_variant&synonymous_variant.vcf\n",
      "Saved: split_vcf_files\\split_splice_region_variant&intron_variant.vcf\n",
      "Saved: split_vcf_files\\split_splice_acceptor_variant&intron_variant.vcf\n",
      "Saved: split_vcf_files\\split_stop_lost.vcf\n",
      "Saved: split_vcf_files\\split_start_lost.vcf\n",
      "Saved: split_vcf_files\\split_missense_variant&splice_region_variant.vcf\n",
      "Saved: split_vcf_files\\split_splice_donor_variant&intron_variant.vcf\n",
      "Saved: split_vcf_files\\split_stop_retained_variant.vcf\n",
      "Saved: split_vcf_files\\split_stop_lost&splice_region_variant.vcf\n",
      "Saved: split_vcf_files\\split_splice_region_variant&non_coding_transcript_exon_variant.vcf\n",
      "Saved: split_vcf_files\\split_splice_donor_variant&splice_region_variant&intron_variant.vcf\n",
      "Saved: split_vcf_files\\split_stop_gained&splice_region_variant.vcf\n",
      "Saved: split_vcf_files\\split_splice_region_variant&stop_retained_variant.vcf\n",
      "Saved: split_vcf_files\\split_initiator_codon_variant.vcf\n",
      "Splitting complete! ðŸš€\n"
     ]
    }
   ],
   "source": [
    "for value in unique_values:\n",
    "    subset_df = df[df[column_20_name] == value]  # Filter rows by value\n",
    "    file_name = os.path.join(output_dir, f\"split_{value}.vcf\")\n",
    "    \n",
    "    # Save while maintaining VCF format\n",
    "    subset_df.to_csv(file_name, sep=\"\\t\", index=False, header=True)\n",
    "\n",
    "    print(f\"Saved: {file_name}\")\n",
    "\n",
    "print(\"Splitting complete! ðŸš€\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
